import os
import io
import json
from datetime import datetime
from typing import List, Optional, Dict, Any

from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field, ValidationError

from langchain.prompts import PromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores.chroma import Chroma
from langchain_experimental.text_splitter import SemanticChunker
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferWindowMemory
from langchain.output_parsers import PydanticOutputParser
from langchain_core.output_parsers import StrOutputParser

from reportlab.pdfgen import canvas
from reportlab.lib.pagesizes import letter
from reportlab.lib.units import inch

# ======================== ENV / LLM ========================
load_dotenv()
os.environ["GOOGLE_API_KEY"] = os.getenv("GEMINI_API_KEY") or ""
if not os.environ["GOOGLE_API_KEY"]:
    raise RuntimeError("GEMINI_API_KEY not found in environment.")

# LLMs
llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro", temperature=0.1)
embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")

# ======================== FASTAPI ==========================
app = FastAPI(title="Medical Chatbot API")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # tighten in prod
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ======================== DB / RAG =========================
CHROMA_DB_PATH = "chroma_db"
KNOWLEDGE_DIR = "knowledge"
PROCESSED_TRACKER = "processed_files.txt"


def build_or_load_vectorstore() -> Chroma:
    os.makedirs(KNOWLEDGE_DIR, exist_ok=True)
    processed = set()
    if os.path.exists(PROCESSED_TRACKER):
        with open(PROCESSED_TRACKER, "r", encoding="utf-8") as f:
            processed = set(l.strip() for l in f if l.strip())

    pdf_files = [f for f in os.listdir(KNOWLEDGE_DIR) if f.lower().endswith(".pdf")]

    def split_docs(docs):
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=1200,
            chunk_overlap=200,
            separators=["\n\n", "\nâ€¢ ", "\n- ", "\n", ". ", " "]
        )
        return splitter.split_documents(docs)

    if not os.path.exists(CHROMA_DB_PATH):
        all_docs = []
        for pdf in pdf_files:
            if pdf in processed:
                continue
            loader = PyPDFLoader(os.path.join(KNOWLEDGE_DIR, pdf))
            all_docs.extend(loader.load())
            processed.add(pdf)

        if all_docs:
            texts = split_docs(all_docs)
            store = Chroma.from_documents(texts, embeddings, persist_directory=CHROMA_DB_PATH)
        else:
            store = Chroma(persist_directory=CHROMA_DB_PATH, embedding_function=embeddings)
    else:
        store = Chroma(persist_directory=CHROMA_DB_PATH, embedding_function=embeddings)
        new_docs = []
        for pdf in pdf_files:
            if pdf in processed:
                continue
            loader = PyPDFLoader(os.path.join(KNOWLEDGE_DIR, pdf))
            new_docs.extend(loader.load())
            processed.add(pdf)
        if new_docs:
            texts = split_docs(new_docs)
            store.add_documents(texts)
            store.persist()

    with open(PROCESSED_TRACKER, "w", encoding="utf-8") as f:
        for name in sorted(processed):
            f.write(name + "\n")

    return store


vector_store = build_or_load_vectorstore()

# ======================== SAFETY PROMPT ====================
qa_prompt = PromptTemplate(
    input_variables=["context", "chat_history", "question"],
    template=(
        "You are a cautious medical assistant. Use ONLY the provided context to answer.\n\n"
        "Rules:\n"
        "- Be clear and concise.\n"
        "- You may suggest POSSIBLE causes but avoid definitive diagnoses.\n"
        "- Give step-by-step self-care/remedies when safe.\n"
        "- Call out RED FLAGS that require urgent care.\n"
        "- Prefer generic OTC names when relevant.\n"
        "- Keep tone reassuring but firm about escalation when needed.\n\n"
        "Context:\n{context}\n\n"
        "Chat History:\n{chat_history}\n\n"
        "User Question:\n{question}\n\n"
        "Answer (short paragraphs; bullets when useful):"
    ),
)

retriever = vector_store.as_retriever(search_kwargs={"k": 5})
memory = ConversationBufferWindowMemory(
    k=100, memory_key="chat_history", return_messages=True, output_key="answer"
)

qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    combine_docs_chain_kwargs={"prompt": qa_prompt},
    return_source_documents=True,
)

# ======================== TRIAGE ===========================
RED_FLAG_PATTERNS = [
    "severe chest pain", "pressure in chest", "shortness of breath", "difficulty breathing",
    "blue lips", "unconscious", "fainted", "confusion", "seizure", "stiff neck",
    "worst headache of my life", "sudden severe headache",
    "weakness on one side", "slurred speech", "facial droop",
    "uncontrolled bleeding", "vomiting blood", "black stools",
    "high fever", "fever above 104", "fever > 104", "fever more than 3 days",
    "infant", "baby", "child under 3 months with fever",
    "severe dehydration", "no urination", "sunken eyes", "lethargic",
    "severe abdominal pain", "rigid abdomen",
    "allergic reaction", "swelling of tongue", "throat closing",
]

def detect_emergency(text: str) -> Optional[str]:
    t = (text or "").lower()
    for p in RED_FLAG_PATTERNS:
        if p in t:
            return (
                "ðŸš¨ **Potential emergency detected.** Please seek urgent in-person care.\n"
                "If you suspect a life-threatening emergency, call your local emergency number immediately."
            )
    return None

# ======================== API SCHEMAS ======================
class ChatRequest(BaseModel):
    query: str

class ChatResponse(BaseModel):
    answer: str
    references: List[str]

# ======= Structured RX schema (what the LLM must return) ===
class Medicine(BaseModel):
    name: str
    adult: str = "â€”"
    child: str = "â€”"
    frequency: str = ""
    duration: str = ""
    notes: str = ""

class RxSchema(BaseModel):
    possible_disease: str
    symptoms: List[str] = Field(default_factory=list)
    remedies: List[str] = Field(default_factory=list)
    medicines: List[Medicine] = Field(default_factory=list)
    # default ensures presence even if model omits it
    safety_note: str = "This is generated by an AI, please consult doctors before following these blindly."

# =================== PDF LAYOUT (ReportLab) =================
def draw_wrapped_text(p, x, y, text, max_width, leading=14, font="Helvetica", size=10):
    from reportlab.pdfbase.pdfmetrics import stringWidth
    p.setFont(font, size)
    words = text.split()
    line = ""
    lines = []
    for w in words:
        test = (line + " " + w).strip()
        if stringWidth(test, font, size) <= max_width:
            line = test
        else:
            if line:
                lines.append(line)
            line = w
    if line:
        lines.append(line)
    for ln in lines:
        p.drawString(x, y, ln)
        y -= leading
    return y

def create_prescription_pdf(payload: Dict[str, Any]) -> io.BytesIO:
    buffer = io.BytesIO()
    p = canvas.Canvas(buffer, pagesize=letter)
    width, height = letter
    left = inch
    right = width - inch
    y = height - inch

    # Header
    p.setFont("Helvetica-Bold", 16)
    p.drawString(left, y, payload["doctor"]["clinic"])
    p.setFont("Helvetica", 10)
    p.drawString(left, y - 18, f"{payload['doctor']['name']}")
    p.drawRightString(right, y, f"Date: {payload['date']}")
    y -= 32
    p.line(left, y, right, y)
    y -= 16

    # Patient
    p.setFont("Helvetica-Bold", 12)
    p.drawString(left, y, "Patient Information")
    y -= 16
    p.setFont("Helvetica", 10)
    p.drawString(left, y, f"Name: {payload['patient']['name']}")
    y -= 14
    age_show = payload['patient'].get('age')
    gender_show = payload['patient'].get('gender') or "â€”"
    p.drawString(left, y, f"Age: {age_show if age_show is not None else 'Not specified'}")
    p.drawString(left + 200, y, f"Gender: {gender_show}")
    y -= 22

    # Assessment
    p.setFont("Helvetica-Bold", 12)
    p.drawString(left, y, "Assessment")
    y -= 14
    p.setFont("Helvetica", 10)
    y = draw_wrapped_text(p, left, y, f"Possible Disease: {payload['possible_disease']}", max_width=right-left)
    y -= 6

    # Symptoms
    if payload["symptoms"]:
        p.setFont("Helvetica-Bold", 12)
        p.drawString(left, y, "Symptoms")
        y -= 14
        p.setFont("Helvetica", 10)
        for s in payload["symptoms"]:
            y = draw_wrapped_text(p, left, y, f"â€¢ {s}", max_width=right-left)
            y -= 2
        y -= 6

    # Remedies / Diagnosis / Treatments
    if payload["remedies"]:
        p.setFont("Helvetica-Bold", 12)
        p.drawString(left, y, "Remedies / Diagnosis / Treatments")
        y -= 14
        p.setFont("Helvetica", 10)
        for r in payload["remedies"]:
            y = draw_wrapped_text(p, left, y, f"â€¢ {r}", max_width=right-left)
            y -= 2
        y -= 6

    # Medicines
    if payload["medicines"]:
        p.setFont("Helvetica-Bold", 12)
        p.drawString(left, y, "Medicines")
        y -= 14
        p.setFont("Helvetica", 10)
        for m in payload["medicines"]:
            name = m.get("name", "Medicine")
            adult = m.get("adult", "â€”")
            child = m.get("child", "â€”")
            freq = m.get("frequency", "")
            dur = m.get("duration", "")
            notes = m.get("notes", "")
            y = draw_wrapped_text(
                p, left, y,
                f"â€¢ {name} â€” Adult: {adult} | Child: {child}"
                + (f" | Frequency: {freq}" if freq else "")
                + (f" | Duration: {dur}" if dur else ""),
                max_width=right-left
            )
            if notes:
                y = draw_wrapped_text(p, left + 14, y, f"Note: {notes}", max_width=right-left-14)
            y -= 4

    # Safety Note
    y -= 8
    p.setFont("Helvetica-Oblique", 9)
    y = draw_wrapped_text(
        p, left, y,
        payload.get("safety_note", "This is generated by an AI; please consult a doctor before following."),
        max_width=right-left, leading=12, font="Helvetica-Oblique", size=9
    )

    p.showPage()
    p.save()
    buffer.seek(0)
    return buffer

# ======================== ENDPOINTS ========================
@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    try:
        result = qa_chain({"question": request.query})
        references = [doc.page_content for doc in result.get("source_documents", [])]
        answer = (result["answer"] or "").strip()

        # Rules-based triage on both user text and answer
        triage = detect_emergency(request.query) or detect_emergency(answer)
        if triage:
            answer += ("\n\n---\n" + triage)

        # Safety nudge
        answer += (
            "\n\n---\n**Note:** Iâ€™m not a substitute for a doctor. "
            "If symptoms are severe, new, or worsening â€” or you suspect an emergency â€” seek in-person care."
        )
        return ChatResponse(answer=answer, references=references)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Chat failed: {e}")

# ======= Structured-output prompt & chain for RX ===========
RX_PARSER = PydanticOutputParser(pydantic_object=RxSchema)

RX_PROMPT = PromptTemplate(
    input_variables=["user_query", "assistant_answer", "age", "gender"],
    partial_variables={"format_instructions": RX_PARSER.get_format_instructions()},
    template=(
        "Prepare a concise, professional prescription summary from the chat.\n"
        "- If age is unknown, ALWAYS include both adult and child guidance in 'medicines'.\n"
        "- Prefer common OTC examples (e.g., Paracetamol/Acetaminophen for fever, ORS for dehydration).\n"
        "- Keep it specific to the query; avoid unrelated diseases.\n"
        "- Keep medicine count small (1â€“4) with practical dosing.\n"
        "- ALWAYS include this sentence in safety_note: "
        "'This is generated by an AI, please consult doctors before following these blindly.'\n\n"
        "{format_instructions}\n\n"
        "User query: {user_query}\n"
        "Assistant answer: {assistant_answer}\n"
        "Patient age (may be None): {age}\n"
        "Patient gender (may be None): {gender}\n"
    ),
)

RX_CHAIN = RX_PROMPT | llm | StrOutputParser() | RX_PARSER

class RxFromChatRequest(BaseModel):
    latest_user_query: str
    assistant_answer: Optional[str] = None
    patient_name: Optional[str] = "Patient"
    patient_age: Optional[int] = None
    patient_gender: Optional[str] = None
    doctor_name: Optional[str] = "Dr. On-call"
    clinic_name: Optional[str] = "Virtual Health"
    date: Optional[str] = datetime.now().strftime("%Y-%m-%d")

@app.post("/generate_prescription_pdf_from_chat")
async def generate_prescription_pdf_from_chat(req: RxFromChatRequest):
    try:
        rx: RxSchema = await RX_CHAIN.ainvoke({
            "user_query": req.latest_user_query,
            "assistant_answer": req.assistant_answer or "",
            "age": str(req.patient_age) if req.patient_age is not None else "None",
            "gender": req.patient_gender or "None",
        })

        payload = {
            "patient": {"name": req.patient_name or "Patient", "age": req.patient_age, "gender": req.patient_gender},
            "doctor": {"name": req.doctor_name or "Dr. On-call", "clinic": req.clinic_name or "Virtual Health"},
            "date": req.date or datetime.now().strftime("%Y-%m-%d"),
            "possible_disease": rx.possible_disease,
            "symptoms": rx.symptoms,
            "remedies": rx.remedies,
            "medicines": [m.model_dump() if hasattr(m, "model_dump") else m.dict() for m in rx.medicines],
            "safety_note": rx.safety_note,
        }

        pdf_buffer = create_prescription_pdf(payload)
        return StreamingResponse(pdf_buffer,
                                 media_type="application/pdf",
                                 headers={"Content-Disposition": "attachment; filename=prescription.pdf"})
    except ValidationError as ve:
        raise HTTPException(status_code=500, detail=f"Model returned invalid schema: {ve}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"PDF generation failed: {type(e).__name__}: {e}")

@app.get("/health")
async def health():
    return JSONResponse({"status": "ok"})

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
